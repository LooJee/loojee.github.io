[{"categories":["golang","design-pattern"],"content":"这次介绍的设计模式是工厂模式，这是一个比较常见的创建型模式。一般情况下，工厂模式分为三种：简单工厂、工厂方法和抽象工厂，下面慢慢举例介绍下。 简单工厂 考虑一个加密程序的应用场景，一个加密程序可能提供了AES，DES等加密方法，这些加密方式都实现了同一个接口ICipher，它有两个方法分别是 Encript 和 Decript。我们使用加密程序的时候会希望简单的指定加密方式，然后传入原始数据以及必要参数，然后就能得到想要的加密数据。这个功能用简单工厂如何实现呢？ ","date":"2021-03-25","objectID":"/posts/design-pattern-with-go-factory/:0:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Factory","uri":"/posts/design-pattern-with-go-factory/"},{"categories":["golang","design-pattern"],"content":"模式结构 简单工厂模式包含一下几个角色： Factory（工厂角色），负责创建所有实例。 Product（抽象产品角色），指工厂所创建的实例的基类，在 golang 中通常为接口。 ConcreteProduce（具体产品），指工厂所创建的具体实例的类型。 在这个加密程序的例子中，工厂角色的职责是返回加密函数；抽象产品角色是所有加密类的基类，在 golang 中是定义了加密类通用方法的接口；具体产品是指具体的加密类，如 AES、DES 等等。我们可以用 UML 关系图来表示这几个角色之间的关系： ","date":"2021-03-25","objectID":"/posts/design-pattern-with-go-factory/:1:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Factory","uri":"/posts/design-pattern-with-go-factory/"},{"categories":["golang","design-pattern"],"content":"代码设计 依据 UML 关系图，我们可以设计出采用简单工厂模式的加密代码。首先是 ICipher 接口，定义了 Encript 和 Decript 两个方法： type ICipher interface { Encrypt([]byte) ([]byte, error) Decrypt([]byte) ([]byte, error) } 然后根据这个接口，分别实现 AESCipher 和 DESCipher 两个加密类。 AESCipher: type AESCipher struct { } func NewAESCipher() *AESCipher { return \u0026AESCipher{} } func (c AESCipher) Encrypt(data []byte) ([]byte, error) { return nil, nil } func (c AESCipher) Decrypt(data []byte) ([]byte, error) { return nil, nil } DESCipher: type DESCipher struct { } func NewDesCipher() *DESCipher { return \u0026DESCipher{} } func (c DESCipher) Encrypt(data []byte) ([]byte, error) { return nil, nil } func (c DESCipher) Decrypt(data []byte) ([]byte, error) { return nil, nil } 最后是一个工厂角色，根据传入的参数返回对应的加密类，Java 需要实现一个工厂类，这里我们用一个函数来做加密类工厂： func CipherFactory(cType string) ICipher { switch cType { case \"AES\": return NewAESCipher() case \"DES\": return NewDesCipher() default: return nil } } 这样，通过调用 CipherFactory 传入所需的加密类型，就可以得到所需要的加密类实例了。 func TestCipherFactory(t *testing.T) { c := CipherFactory(\"RSA\") if c != nil { t.Fatalf(\"unsupport RSA\") } c = CipherFactory(\"AES\") if reflect.TypeOf(c) != reflect.TypeOf(\u0026AESCipher{}) { t.Fatalf(\"cipher type should be AES\") } c = CipherFactory(\"DES\") if reflect.TypeOf(c) != reflect.TypeOf(\u0026DESCipher{}) { t.Fatalf(\"cipher type should be DES\") } } ","date":"2021-03-25","objectID":"/posts/design-pattern-with-go-factory/:2:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Factory","uri":"/posts/design-pattern-with-go-factory/"},{"categories":["golang","design-pattern"],"content":"小结 简单工厂将业务代码和创建实例的代码分离，使职责更加单一。不过，它将所有创建实例的代码都放到了 CipherFactory 中，当加密类增加的时候会增加工厂函数的复杂度，产品类型增加时需要更新工厂函数这一操作也是违反了“开闭原则”，所以简单工厂更适合负责创建的对象比较少的场景。 工厂方法 为了让代码更加符合“开闭原则”，我们可以给每个产品都增加一个工厂子类，每个子类生成具体的产品实例，将工厂方法化，也就是现在要介绍的工厂方法模式。 ","date":"2021-03-25","objectID":"/posts/design-pattern-with-go-factory/:3:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Factory","uri":"/posts/design-pattern-with-go-factory/"},{"categories":["golang","design-pattern"],"content":"模式结构 工厂方法和和简单工厂相比，将工厂角色细分成抽象工厂和具体工厂： Product（抽象产品）：定义产品的接口。 ConcreteFactory（具体产品）：具体的产品实例。 Factory（抽象工厂）：定义工厂的接口。 ConcreteFactory（具体工厂）：实现抽象工厂，生产具体产品。 可以使用如下的 UML 图来表示这几个角色直接的关系： ","date":"2021-03-25","objectID":"/posts/design-pattern-with-go-factory/:4:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Factory","uri":"/posts/design-pattern-with-go-factory/"},{"categories":["golang","design-pattern"],"content":"代码设计 抽象产品角色和具体产品角色就不再定义了，和简单工厂相同，具体展示一下抽象工厂角色和具体工厂角色。 抽象工厂角色定义了一个方法，用于创建对应的产品： type ICipherFactory interface { GetCipher() ICipher } 根据这个接口，分别定义出 AESCipherFactory、和 DESCipherFactory 两个子类工厂。 AESCipherFactory type AESCipherFactory struct { } func (AESCipherFactory) GetCipher() ICipher { return NewAESCipher() } func NewAESCipherFactory() *AESCipherFactory { return \u0026AESCipherFactory{} } DESCipherFactory type DESCipherFactory struct { } func (DESCipherFactory) GetCipher() ICipher { return NewDESCipher() } func NewDESCipherFactory() *DESCipherFactory { return \u0026DESCipherFactory{} } 然后编写一个单元测试来检验我们的代码： func TestCipherFactory(t *testing.T) { var f ICipherFactory = NewAESCipherFactory() if reflect.TypeOf(f.GetCipher()) != reflect.TypeOf(\u0026AESCipher{}) { t.Fatalf(\"should be AESCipher\") } f = NewDESCipherFactory() if reflect.TypeOf(f.GetCipher()) != reflect.TypeOf(\u0026DESCipher{}) { t.Fatalf(\"should be DESCipher\") } } ","date":"2021-03-25","objectID":"/posts/design-pattern-with-go-factory/:5:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Factory","uri":"/posts/design-pattern-with-go-factory/"},{"categories":["golang","design-pattern"],"content":"小结 在工厂方法模式中，定义了一个工厂接口，然后根据各个产品子类定义实现这个接口的子类工厂，通过子类工厂来返回产品实例。这样修改创建实例代码只需要修改子类工厂，新增实例时只需要新增具体工厂和具体产品，而不需要修改其它代码，符合“开闭原则”。不过，当具体产品较多的时候，系统中类的数量也会成倍的增加，一定程度上增加了系统的复杂度。而且，在实际使用场景中，可能还需要使用反射等技术，增加了代码的抽象性和理解难度。 抽象工厂 下面再用加密这个例子可能不太好，不过我们假设需求都合理吧。现在需求更加细化了，分别需要 64 位 key 和 128 位 key 的 AES 加密库以及 64 位 key 和 128 位 key 的 DES 加密库。如果使用工厂方法模式，我们一共需要定义 4 个具体工厂和 4 个具体产品。 AESCipher64 AESCipher128 AESCipherFactory64 AESCipherFactory128 DESCipher64 DESCipher128 DESCipherFactory64 DESCipherFactory128 这时候，我们可以把有关联性的具体产品组合成一个产品组，例如AESCipher64 和 AESCipher128，让它们通过同一个工厂 AESCipherFactory 来生产，这样就可以简化成 2 个具体工厂和 4 个具体产品 AESCipher64 AESCipher128 AESCipherFactory DESCipher64 DESCipher128 DESCipherFactory 这就是抽象工厂模式。 ","date":"2021-03-25","objectID":"/posts/design-pattern-with-go-factory/:6:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Factory","uri":"/posts/design-pattern-with-go-factory/"},{"categories":["golang","design-pattern"],"content":"模式结构 抽象工厂共有 4 个角色： AbstractFactory（抽象工厂）：定义工厂的接口。 ConcreteFactory（具体工厂）：实现抽象工厂，生产具体产品。 AbstractProduct（抽象产品）：定义产品的接口。 Product（具体产品）：具体的产品实例。 根据角色定义我们可以画出抽象工厂的 UML 关系图： ","date":"2021-03-25","objectID":"/posts/design-pattern-with-go-factory/:7:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Factory","uri":"/posts/design-pattern-with-go-factory/"},{"categories":["golang","design-pattern"],"content":"代码设计 抽象产品和具体产品的定义与工厂方法类似： 抽象产品： type ICipher interface { Encrypt(data, key[]byte) ([]byte, error) Decrypt(data, key[]byte) ([]byte, error) } AESCipher64： type AESCipher64 struct { } func NewAESCipher64() *AESCipher64 { return \u0026AESCipher64{} } func (AESCipher64) Encrypt(data, key []byte) ([]byte, error) { return nil, nil } func (AESCipher64) Decrypt(data, key []byte) ([]byte, error) { return nil, nil } AESCipher128： type AESCipher128 struct { } func NewAESCipher128() *AESCipher128 { return \u0026AESCipher128{} } func (AESCipher128) Encrypt(data, key []byte) ([]byte, error) { return nil, nil } func (AESCipher128) Decrypt(data, key []byte) ([]byte, error) { return nil, nil } AESCipher128： type c struct { } func NewDESCipher64() *DESCipher64 { return \u0026DESCipher64{} } func (DESCipher64) Encrypt(data, key []byte) ([]byte, error) { return nil, nil } func (DESCipher64) Decrypt(data, key []byte) ([]byte, error) { return nil, nil } DESCipher128： type DESCipher128 struct { } func NewDESCipher128() *DESCipher128 { return \u0026DESCipher128{} } func (DESCipher128) Encrypt(data, key []byte) ([]byte, error) { return nil, nil } func (DESCipher128) Decrypt(data, key []byte) ([]byte, error) { return nil, nil } 抽象工厂角色和工厂方法相比需要增加 GetCipher64 和 GetCipher128 两个方法定义： type ICipherFactory interface { GetCipher64() ICipher GetCipher128() ICipher } 然后分别实现 AESCipherFactory 和 DesCipherFactory 两个具体工厂： AESCipherFactory： type AESCipherFactory struct { } func (AESCipherFactory) GetCipher64() ICipher { return NewAESCipher64() } func (AESCipherFactory) GetCipher128() ICipher { return NewAESCipher128() } func NewAESCipherFactory() *AESCipherFactory { return \u0026AESCipherFactory{} } DESCipherFactory： type DESCipherFactory struct { } func (DESCipherFactory) GetCipher64() ICipher { return NewDESCipher64() } func (DESCipherFactory) GetCipher128() ICipher { return NewDESCipher128() } func NewDESCipherFactory() *DESCipherFactory { return \u0026DESCipherFactory{} } 编写单元测试验证我们的代码： func TestAbstractFactory(t *testing.T) { var f = NewCipherFactory(\"AES\") if reflect.TypeOf(f.GetCipher64()) != reflect.TypeOf(\u0026AESCipher64{}) { t.Fatalf(\"should be AESCipher64\") } if reflect.TypeOf(f.GetCipher128()) != reflect.TypeOf(\u0026AESCipher128{}) { t.Fatalf(\"should be AESCipher128\") } f = NewCipherFactory(\"DES\") if reflect.TypeOf(f.GetCipher64()) != reflect.TypeOf(\u0026DESCipher64{}) { t.Fatalf(\"should be DESCipher64\") } if reflect.TypeOf(f.GetCipher128()) != reflect.TypeOf(\u0026DESCipher128{}) { t.Fatalf(\"should be DESCipher128\") } } ","date":"2021-03-25","objectID":"/posts/design-pattern-with-go-factory/:8:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Factory","uri":"/posts/design-pattern-with-go-factory/"},{"categories":["golang","design-pattern"],"content":"小结 抽象工厂模式也符合单一职责原则和开闭原则，不过需要引入大量的类和接口，使代码更加复杂。并且，当增加新的具体产品时，需要修改抽象工厂和所有的具体工厂。 总结 今天介绍了创建型模式之工厂模式，工厂模式包括简单工厂、工厂方法和抽象工厂。简单工厂的复杂性比较低，但是不像工厂方法和抽象工厂符合单一职责原则和开闭原则。实际使用时，通常会选择符合开闭原则，复杂度也不是特别高的工厂方法。如果有特别需求可以选择使用抽象工厂。 ","date":"2021-03-25","objectID":"/posts/design-pattern-with-go-factory/:9:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Factory","uri":"/posts/design-pattern-with-go-factory/"},{"categories":["golang","design-pattern"],"content":"定义 一个类只允许创建一个对象（或者实例），那这个类就是一个单例类，这种设计模式就叫作单例模式。当某些数据只需要在系统中保留一份的时候，可以选择使用单例模式。 ","date":"2021-03-22","objectID":"/posts/design-pattern-with-go-singleton/:1:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Singleton","uri":"/posts/design-pattern-with-go-singleton/"},{"categories":["golang","design-pattern"],"content":"饿汉式 饿汉式的实现方式比较简单。在类加载的时候，静态实例就已经创建并初始化好了，所以，实例的创建过程是线程安全的。如果实例占用资源多，按照 fail-fast 的设计原则（有问题及早暴露），那我们也希望在程序启动时就将这个实例初始化好。如果资源不够，就会在程序启动的时候触发报错，我们可以立即去修复。这样也能避免在程序运行一段时间后，突然因为初始化这个实例占用资源过多，导致系统崩溃，影响系统的可用性。 下面是一个典型的饿汉式单例模式： package singleton import \"sync/atomic\" type HungrySingleton struct { data uint64 } var hungry = \u0026HungrySingleton{} func HungrySingletonInstance() *HungrySingleton { return hungry } ","date":"2021-03-22","objectID":"/posts/design-pattern-with-go-singleton/:2:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Singleton","uri":"/posts/design-pattern-with-go-singleton/"},{"categories":["golang","design-pattern"],"content":"懒汉式 懒汉式单例模式可以延迟加载类实例，如果想要在使用类实例的时候再创建，可以采用这种方式来实现单例模式。不过如果类实例初始化时间比较长，可能会对效率有一定的影响。 package singleton import ( \"sync\" \"sync/atomic\" ) type LazySingleton struct { data uint64 } var lazySingleton *LazySingleton var lock sync.Mutex func init() { lock = sync.Mutex{} } func LazySingletonInstance() *LazySingleton { lock.Lock() defer lock.Unlock() if lazySingleton == nil { lazySingleton = \u0026LazySingleton{} } return lazySingleton } ","date":"2021-03-22","objectID":"/posts/design-pattern-with-go-singleton/:3:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Singleton","uri":"/posts/design-pattern-with-go-singleton/"},{"categories":["golang","design-pattern"],"content":"双重检测懒汉式 上面面这种实现方式比较简单粗暴，每次获取实例的时候都需要加锁，这会大大增加时间开销，效率十分低下。我们可以改进下，创建实例的时候，先判断实例是否已经创建，如果没有创建再进入创建流程，这样会减少等锁的次数，增加效率。 package singleton import \"sync\" type LockCheckLazySingleton struct { data int64 } var ll sync.Mutex var lcSingleton *LockCheckLazySingleton func init() { ll = sync.Mutex{} } func LockCheckLazySingletonInstance() *LockCheckLazySingleton { if lcSingleton == nil { ll.Lock() defer ll.Unlock() if lcSingleton == nil { lcSingleton = \u0026LockCheckLazySingleton{} } } return lcSingleton } 在 golang 中，我们可以使用 sync.Once 来简化判断流程： package singleton import ( \"sync\" \"sync/atomic\" ) type CheckLazySingleton struct { data uint64 } var checkLazySingleton *CheckLazySingleton var checkOnce = sync.Once{} func CheckLazySingletonInstance() *CheckLazySingleton { if checkLazySingleton == nil { checkOnce.Do(func() { checkLazySingleton = \u0026CheckLazySingleton{} }) } return checkLazySingleton } sync.Once 使用原子操作来模拟锁的效果，来判断实例是否已经创建，和 LockCheckLazySingleton 中直接判断实例相比，并没有太多的性能开销，反而还可以避免并发时更多的运行实例进入等锁的状态，这种方式理论上效率会比上面那种方式更高一点。 ","date":"2021-03-22","objectID":"/posts/design-pattern-with-go-singleton/:4:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Singleton","uri":"/posts/design-pattern-with-go-singleton/"},{"categories":["golang","design-pattern"],"content":"性能测试 我们可以编写一个 benchmark 测试一下这几种单例模式实现方式的效率怎么样： func BenchmarkHungrySingletonInstance(b *testing.B) { b.RunParallel(func(pb *testing.PB) { for pb.Next() { if singleton.HungrySingletonInstance() != singleton.HungrySingletonInstance() { b.Errorf(\"different instance\") } } }) } func BenchmarkLazySingletonInstance(b *testing.B) { b.RunParallel(func(pb *testing.PB) { for pb.Next() { if singleton.LazySingletonInstance() != singleton.LazySingletonInstance() { b.Errorf(\"different instance\") } } }) } func BenchmarkLockCheckLazySingletonInstance(b *testing.B) { b.RunParallel(func(pb *testing.PB) { for pb.Next() { if singleton.LockCheckLazySingletonInstance() != singleton.LockCheckLazySingletonInstance() { b.Errorf(\"different instance\") } } }) } func BenchmarkCheckLazySingletonInstance(b *testing.B) { b.RunParallel(func(pb *testing.PB) { for pb.Next() { if singleton.CheckLazySingletonInstance() != singleton.CheckLazySingletonInstance() { b.Errorf(\"different instance\") } } }) } 输出结果如下所示： D:\\git\\design-pattern\\singleton\u003ego test -bench=. goos: windows goarch: amd64 pkg: design-pattern/singleton cpu: Intel(R) Core(TM) i5-7200U CPU @ 2.50GHz BenchmarkHungrySingletonInstance-4 1000000000 0.5729 ns/op BenchmarkLazySingletonInstance-4 17065185 71.58 ns/op BenchmarkLockCheckLazySingletonInstance-4 377396236 2.878 ns/op BenchmarkCheckLazySingletonInstance-4 536262001 2.330 ns/op PASS ok design-pattern/singleton 4.733s 我们可以看到，饿汉模式的效率是最高的，因为它的实例是在程序启动的时候就已经初始化好了，调用实例的过程省去了很多判断的过程。而直接加锁的懒汉模式是效率最低的，锁的开销是十分大的，实际使用中，如果有需要使用懒汉式的单例模式，也不推荐这种实现方式。后面两种优化的懒汉模式中，使用 once 的效率会更高一点，这种实现方式更加值得推荐。 ","date":"2021-03-22","objectID":"/posts/design-pattern-with-go-singleton/:5:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Singleton","uri":"/posts/design-pattern-with-go-singleton/"},{"categories":["golang","design-pattern"],"content":"单例模式的问题及替代方案 ","date":"2021-03-22","objectID":"/posts/design-pattern-with-go-singleton/:6:0","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Singleton","uri":"/posts/design-pattern-with-go-singleton/"},{"categories":["golang","design-pattern"],"content":"单例模式的问题 单例对 OOP 特性（封装、继承、多态、抽象）的支持不友好。单例模式违反了基于接口而非实现的设计原则。 单例会隐藏类之间的依赖关系。单例类不需要显示创建、不需要依赖参数传递，在代码比较复杂的情况下，调用关系就会非常隐蔽。 单例对代码的扩展性不友好，在某些情况下会影响代码的扩展性、灵活性。 单例对代码的可测试性不友好。单例类持有的变量通常是全局变量，被所有的代码共享，这不方便编写单元测试。 单例不支持有参数的构造函数，因为单例类的对象只初始化一次，即使构造函数有参数也只接收第一次的参数。 ","date":"2021-03-22","objectID":"/posts/design-pattern-with-go-singleton/:6:1","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Singleton","uri":"/posts/design-pattern-with-go-singleton/"},{"categories":["golang","design-pattern"],"content":"替代方案 可以通过工厂模式、IOC容器（JAVA）等方式 ","date":"2021-03-22","objectID":"/posts/design-pattern-with-go-singleton/:6:2","tags":["golang","design-pattern"],"title":"Design Pattern With Go: Singleton","uri":"/posts/design-pattern-with-go-singleton/"},{"categories":null,"content":"TL;DR 是因为创建的容器采用的网络类型是 bridge，而宿主机没有启用 ip 转发，所以，外部主机请求没有转发到对应容器。需要两个步骤启用 ip 转发功能。（来自 Docker 官方文档 ） 配置 Linux 内核允许 ip 转发。 $ sysctl net.ipv4.conf.all.forwarding=1 将 iptables FORWARD 规则从 DROP 改为 ACCEPT。 $ sudo iptables -P FORWARD ACCEPT ","date":"2021-02-01","objectID":"/forwarding_from_docker_containers_to_the_outside_world.html:0:0","tags":["docker"],"title":"外部主机无法访问宿主机内容器是怎么回事","uri":"/forwarding_from_docker_containers_to_the_outside_world.html"},{"categories":null,"content":"短话长说 ","date":"2021-02-01","objectID":"/forwarding_from_docker_containers_to_the_outside_world.html:1:0","tags":["docker"],"title":"外部主机无法访问宿主机内容器是怎么回事","uri":"/forwarding_from_docker_containers_to_the_outside_world.html"},{"categories":null,"content":"背景 因为测试需要，在一台 centos 7 服务器上用 Docker 部署了一个 nginx 服务。部署好之后，在宿主机上使用 curl 命令可以访问 nginx 服务，但是外部机访问却提示 HTTP ERROR 503，服务不可用。查看了容器中 nginx 的 access 日志，发现也没有访问日志，双方都表示不是自己的责任。那就看看是不是中间商（宿主机）的责任吧。 ","date":"2021-02-01","objectID":"/forwarding_from_docker_containers_to_the_outside_world.html:1:1","tags":["docker"],"title":"外部主机无法访问宿主机内容器是怎么回事","uri":"/forwarding_from_docker_containers_to_the_outside_world.html"},{"categories":null,"content":"排查问题 首先想到的是宿主机的防火墙可能没有开放端口，把请求给拦掉了。不过，使用 systemctl status firewalld查看防火墙并没有开启。那就看看是不是 Docker 自身的网络配置的问题。 首先使用 docker inspect docker-name查看容器的网络配置信息： \"Networks\": { \"root_default\": { \"IPAMConfig\": null, \"Links\": null, \"Aliases\": [ \"survey-nginx\", \"dd6d4af0b1f0\" ], \"NetworkID\": \"9781d8e96e344e662a0397a7b25f82ef6bce7e650b035b46404557438af0b5fb\", \"EndpointID\": \"e62f1ce9da0fb9c0668986cfab01c1edc825f1c95cdfa042a6d5becb005cd15e\", \"Gateway\": \"172.19.0.1\", \"IPAddress\": \"172.19.0.2\", \"IPPrefixLen\": 16, \"IPv6Gateway\": \"\", \"GlobalIPv6Address\": \"\", \"GlobalIPv6PrefixLen\": 0, \"MacAddress\": \"02:42:ac:13:00:02\" } } 可以看到使用的网络类型是 root_default，一个自定义的网络类型，继续使用命令 docker network ls查看该网络具体的网络类型： NETWORK ID NAME DRIVER SCOPE 5934ab852fe2 bridge bridge local 057efb45b2a8 host host local 8bb8b2aa1a26 kind bridge local a99c8640a808 none null local 9781d8e96e34 root_default bridge local 如上所示，自定义网络类型 root_defualt 是网桥，那么网桥为什么会导致数据没有转发呢？我的 Docker 知识比较匮乏，不过可以到官方文档看看有没有相应的描述。（文档传送） 在文档中，看到了这么一段内容： In terms of Docker, a bridge network uses a software bridge which allows containers connected to the same bridge network to communicate, while providing isolation from containers which are not connected to that bridge network. The Docker bridge driver automatically installs rules in the host machine so that containers on different bridge networks cannot communicate directly with each other. Bridge networks apply to containers running on the same Docker daemon host. For communication among containers running on different Docker daemon hosts, you can either manage routing at the OS level, or you can use an overlay network. 从文档中，我们可以了解到，Docker 的网桥是软件网桥，它只允许使用了同一个网桥的容器之间互相通信，Docker 会自动在宿主机上创建转发规则，避免不同网桥上的容器相互直接通信。不过，如果是不同宿主机上的容器，需要在系统层面增加路由，或者将网络类型改为 overlay。似乎已经破案了，虽然文档中说的是不同主机间容器的通信，不同主机和当前宿主机的容器间的通信可能也是因为现在这台主机上可能没有开启系统层面的路由。 文档继续往下看，我们找到了开启系统方法，即本文开始的两条命令： $ sysctl net.ipv4.conf.all.forwarding=1 $ sudo iptables -P FORWARD ACCEPT 因为，宿主机没有开启防火墙，所以只执行了第一条命令，然后通过浏览器访问服务，访问成功。 ","date":"2021-02-01","objectID":"/forwarding_from_docker_containers_to_the_outside_world.html:1:2","tags":["docker"],"title":"外部主机无法访问宿主机内容器是怎么回事","uri":"/forwarding_from_docker_containers_to_the_outside_world.html"},{"categories":null,"content":"定义类型、声明变量、使用变量是一门编程语言的基本功能，我们可以这样来定义一个结构体类型： type Foo struct { X string `foo:\"x\"` Y int `foo:\"y\"` } 像这样来使用这个类型声明一个变量： var bar Foo 使用变量也很方便，像这样就可以在终端打印出结构体的字段了: fmt.Printf(\"%s, %s\", bar.X, bar.Y) 以上这些操作都是在明确了变量的类型的时候进行的，不过在编程过程中，我们可能会遇到一种情况：在编写代码时无法明确变量的类型，变量的信息只有在程序运行时在会获取，比如这样的函数： func theFunc(val interface{}) 它的函数签名只有一个参数类型为 interface 的参数 val，这意味着可以将任意类型的变量作为实参传入函数。这样的情况下该如何操作变量 val 呢？golang 标准库中有一个 reflect包–即反射–它提供了一系列的方法能帮助我们在运行时获取变量的信息，或者修改变量的值。在反射中有三个比较重要的概念：Type、Kind 和 Value。下面就一起来看看反射的奇妙之处吧。 如果我们把 bar 变量传入了 theFunc 函数，在函数中我们需要知道些什么信息呢？可能会需要知道传入的变量是什么类型的，如果是一个结构体，可能还会需要知道结构体中有那些字段，这些字段又是什么类型的。我们可能还需要根据结构体字段特定的 tag 来执行特定的操作，比如 encoding/json 包就会根据 tag 来给序列化的 json 字段取名。如果传入的变量是我们所期望的，我们可能还需要修改它的值，或者用它来创建一个新的变量。我们就一个个来谈谈如何用反射来实现这些功能把。 ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:0:0","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"获取变量类型 ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:1:0","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"TypeOf() reflect 提供了 TypeOf 函数来获取指定变量的类型，它的返回值类型为 reflect.Type，这是一个接口类型，它提供了一系列的方法来获取变量相关信息的方法。 ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:1:1","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"Name()、Kind() 和 Elem() Name() 方法获取变量的类型名称，不过它有一个限制，即只能获取基本类型或者自定义的结构体的类型名称，其他的类型会返回一个空的字符串。 Kind() 方法会返回变量的内置类型名称，比如 ptr、slice、array、map、func、struct 等等。它通常可以和 switch 配合来做类型判断。 Elem() 方法用于判断类型的元素类型（type’s element type）。它是对 Name 方法的补充，它可以返回 array, chan, map, ptr, 或 slice 类型中元素的类型。比如，针对一个指针 \u0026bar， Elem 方法会返回 Foo 这个类型名称；针对 []string 这样一个字符串 slice，Elem 会返回 string 这个类型名称。如果不在允许的类型上调用 Elem 方法，会导致 panic ，其实 reflect 包中很多方法和函数都是这样的，它要求使用者知道自己在做什么。 下面来举一个完整的示例吧： import ( \"fmt\" \"reflect\" ) type Foo struct { X string `foo:\"x\"` Y string `foo:\"y\"` } func main() { bar := Foo{ X: \"hello\", Y: \"world\", } sli := make([]string, 0) ch := make(chan bool) m := make(map[int]int) arr := [10]int{} i := 0 f := 1.1 b := true theFunc(bar) theFunc(\u0026bar) theFunc(sli) theFunc(ch) theFunc(m) theFunc(arr) theFunc(theFunc) theFunc(i) theFunc(f) theFunc(b) } func theFunc(val interface{}) { valType := reflect.TypeOf(val) fmt.Printf(\"name of value : %s, kind of value : %s, \", valType.Name(), valType.Kind()) switch valType.Kind() { case reflect.Ptr, reflect.Array, reflect.Chan, reflect.Slice, reflect.Map: fmt.Printf(\"elem of value : %s\", valType.Elem()) } fmt.Println() } 输出为： ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:1:2","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"获取变量值 ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:2:0","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"ValueOf() ValueOf() 函数获取变量中实际存储的值。例如： bar := Foo { X: \"hello\", Y: \"world\", } fmt.Println(reflect.ValueOf(bar)) 输出的结果为: {\"hello\", \"world\"} ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:2:1","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"类型断言 如果使用者知道传入的值是什么类型的，或者通过类型判断的方式获取了值的类型信息，那么可以使用 val.(type) 这种类型断言的方式将传入的 interface 类型的数据强制转换成我们所需要的类型，这时候就可以正常使用类型的字段了。需要注意的是，如果类型断言出错了，那么将会引发 panic，所以，在使用的时候可以先确认变量的类型再进行类型断言，或者利用类型断言的第二个布尔类型的返回值来判断断言是否成功。 func main() { bar := Foo { X: \"hello\", Y: \"world\", } theFunc(bar) } func theFunc(val interface{}) { // v1 := val.(int) // 引发 panic //在使用断言前先判断类型是否正确 if reflect.TypeOf(val) == reflect.TypeOf(Foo{}) { v2 := val.(Foo) fmt.Println(v2) } //使用 ok 来判断断言是否成功 if v2, ok := val.(Foo); ok { fmt.Println(v2) } } 输出为 {\"hello\" \"world\"} {\"hello\" \"world\"} ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:2:2","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"遍历类型的方法 NumMethod()、Method(int)、 MethodByName(string) 这几个方法可以获取变量所对应的类型已导出的方法： type Foo struct { X string `foo:\"x\"` Y string `foo:\"y\"` } func (Foo) unExported() { fmt.Println(\"unExported\") } func (Foo) Exported() { fmt.Println(\"Exported\") } func main() { bar := Foo{ X: \"hello\", Y: \"world\", } valTheFun(bar) } func valTheFun(val interface{}) { valType := reflect.TypeOf(val) fmt.Println(valType.NumMethod()) for i := 0; i \u003c valType.NumMethod(); i++ { fmt.Println(valType.Method(i).Name) } } 输出结果： 1 Exported ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:2:3","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"遍历函数的入参和出参 对于函数类型，Type 接口也提供了相应的方法来遍历入参和出参：NumIn()，In(i int), NumOut，Out(i int)。这几个方法只能用与类型为函数的变量，否则将会引发 panic 。 func main() { valTheFun(valTheFun) valTheFun(param) } func param(i int, s string) (int, string) { return i, s } func valTheFun(val interface{}) { valType := reflect.TypeOf(val) fmt.Printf(\"number of in args %d:\\n\", valType.NumIn()) for i := 0; i \u003c valType.NumIn(); i++ { fmt.Printf(\"\\t %s\\n\", valType.In(i)) } fmt.Printf(\"number of out args %d:\\n\", valType.NumOut()) for i := 0; i \u003c valType.NumOut(); i++ { fmt.Printf(\"\\t %s\\n\", valType.Out(i)) } } 输出： number of in args 1: interface {} number of out args 0: number of in args 2: int string number of out args 2: int string ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:2:4","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"遍历结构体的字段 遍历结构体的字段是十分常用的，当我们需要统一处理 kind 为结构体的入参，但又不知道结构体的具体类型，也不需要知道结构体的具体类型的时候，就可以用上遍历结构体字段的一系列方法了。结构体中还有一个 tag 属性，用它可以给结构体中的字段添加额外的属性，golang 也提供了方法用于遍历 tag 的数据。个人觉得这一部分的功能还是需要好好研究一下的，熟悉这部分的操作可以写出更好的代码，golang 标准库中比较常用的场景有： encoding/json 将结构体序列化为 json 字符串；encoding/xml 将结构体序列化为 xml 数据等等。下面给一个简单的例子： func main() { bar := Foo{ X: \"hello\", Y: \"world\", } structFunc(bar) } func structFunc(val interface{}) { valType := reflect.TypeOf(val) fmt.Println(\"number of fields in val :\", valType.NumField()) for i := 0; i \u003c valType.NumField(); i++ { fmt.Printf(\"field : %s \", valType.Field(i).Name) fmt.Println(\"tags\", valType.Field(i).Tag) } } 输出： number of fields in val : 2 field : X tags foo:\"x\" field : Y tags foo:\"y\" ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:2:5","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"创建新的变量 reflect 中有两种函数可以创建新的变量，分别是 New(Type) 和 Make* 。用两种是因为 Make* 是一系列的函数，它们和内建函数 make 一样，只能为 slice, map, chan 来创建新的变量，不同的是 reflect 为这几个类型都分别声明了一个 Make 函数，并且还为 func 类型也声明了一个 Make 函数。而 New 和内建的 new 函数一样，返回的是创建的变量的指针，这个很重要。因为给新建的变量设置值的时候，需要使用 Field() 方法来指定结构体中的字段，而这个方法的接收器必须为 struct，所以，新建的变量必须要先调用 Elem() 方法来获取对应的结构体类型，然后再调用 Field() 方法来设置新的值。 func main() { bar := Foo{ X: \"hello\", Y: \"world\", } valType := reflect.TypeOf(bar) valFields := reflect.ValueOf(bar) val := reflect.New(valType) //因为 val 是一个指针，所以需要使用 Elem 来获取元素的实际类型 val.Elem().Field(0).SetString(valFields.Field(0).String()) val.Elem().Field(1).SetString(\"golang\") //val 是一个 reflect.Value 类型的变量， //需要通过 Interface() 来获取它所维护的数据， //然后再通过类型断言强制转换为指定的类型 if v, ok := val.Interface().(*Foo); !ok { panic(\"wrong type\") } else { fmt.Println(*v) } } 输出： {hello golang} ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:2:6","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"小结 到这里，基本是把 reflect 的基本操作都讲了一遍了，本来是想简单写写的，结果越写越多。可能有些方面没有将的十分清楚，请各位看官多多斧正。 ","date":"2019-09-13","objectID":"/first_meet_golang_reflect.html:3:0","tags":["golang"],"title":"初识golang中的反射","uri":"/first_meet_golang_reflect.html"},{"categories":null,"content":"前面有说到，redigo 不是一个并发安全的 redis 库，它推荐在并发时使用连接池来访问 redis 服务器，redigo 连接池的典型使用方法如下所示： package main import ( \"github.com/gomodule/redigo/redis\" \"log\" ) func main() { pool := redis.Pool{ Dial: func() (conn redis.Conn, e error) { return redis.Dial(\"tcp\",\"192.168.1.10:6379\") }, MaxIdle:2, } defer pool.Close() conn := pool.Get() reply, err := redis.String(conn.Do(\"SET\", \"hello\", \"world\")) if err != nil { log.Println(err) } log.Println(reply) conn.Close() } ","date":"2019-06-23","objectID":"/redigo_pool.html:0:0","tags":["golang"],"title":"读源码:redigo 的连接池","uri":"/redigo_pool.html"},{"categories":null,"content":"Pool 结构说明 在介绍 pool 几个重要方法的实现之前，我们先来看一下 redis.Pool 结构的一些参数，godoc 的传送门在这里。 参数名 说明 Dial 该参数为链接redis的函数，每次从连接池中获取连接时，如果没有空闲的链接，就将会用该函数创建新的链接 TestOnBorrow 测试连接状态的函数 MaxIdle 连接池最大可有的空闲连接数 MaxActive 连接池最大可有的连接数，这个参数通常和下面的Wait参数同时使用 IdleTimeout 空闲连接的超时时间 Wait 当该值为True，并且设置了MaxActive，当已使用的连接数已经达到了MaxActive，那么，Get函数会一直等待，直到有连接可用；如果设置了MaxActive，但是Wait为False，那么，当没有连接可用时，会直接返回 ErrPoolExhausted 的错误。 MaxConnLifetime 连接的生命周期，当调用Get时，会判断连接是否超时，如果超时，会将连接关闭 chInitialized 该值用来判断下面的ch参数是否已经初始化 mu 互斥锁就不需要介绍了 closed 判断连接池是否已经关闭 active 记录连接池中的活跃连接数，该值和下面的idle可以通过方法 Stats() 来查看 ch 该值和Wait参数配合使用，它会被初始化成缓冲区长度和MaxIdle相同的channel，每当使用一个连接时，就会在ch中写入一个值，当使用的连接数和MaxIdle相等时，ch就会阻塞，直到有连接回收 idle 连接池中空闲的连接数，可以通过方法 Stats() 来查看。它是一个链表，它的结构是这样的：type idleList struct {count int; front, back *poolConn;} ","date":"2019-06-23","objectID":"/redigo_pool.html:1:0","tags":["golang"],"title":"读源码:redigo 的连接池","uri":"/redigo_pool.html"},{"categories":null,"content":"Get方法 当新建一个连接池之后，我们使用方法 Get 从连接池中取出一个连接来进行相关的操作，Get 方法的实现如下所示： func (p *Pool) Get() Conn { pc, err := p.get(nil) if err != nil { return errorConn{err} } return \u0026activeConn{p: p, pc: pc} } 如果能成功通过 get 方法获取连接，则返回 activeConn 的实例，否则，返回 errConn 的实例。如果，返回的是 errConn 的实例的话，那么，不论调用什么方法都会直接返回错误 err，十分巧妙的设计。 现在来看一下 get 方法的实现。get 方法的第一段代码如下所示： if p.Wait \u0026\u0026 p.MaxActive \u003e 0 { p.lazyInit() if ctx == nil { \u003c-p.ch } else { select { case \u003c-p.ch: case \u003c-ctx.Done(): return nil, ctx.Err() } } } 这段代码只有在 p.Wait 为 True 并且设置了 MaxActive 的时候才会执行，它会在 lazyInit（实现如下）中初始化 p.ch ，并且往 p.ch 中写满数据，这样就可以保证能获取 MaxActive 个连接，多余的 Get 请求将会阻塞在 \u003c-p.ch，直到 p.ch 中写入数据，即有连接空闲。 unc (p *Pool) lazyInit() { // Fast path. if atomic.LoadUint32(\u0026p.chInitialized) == 1 { return } // Slow path. p.mu.Lock() if p.chInitialized == 0 { p.ch = make(chan struct{}, p.MaxActive) if p.closed { close(p.ch) } else { for i := 0; i \u003c p.MaxActive; i++ { p.ch \u003c- struct{}{} } } atomic.StoreUint32(\u0026p.chInitialized, 1) } p.mu.Unlock() } get 方法中第二段代码是用来判断连接池中是否有超时的空闲连接，先来看一下代码： if p.IdleTimeout \u003e 0 { n := p.idle.count for i := 0; i \u003c n \u0026\u0026 p.idle.back != nil \u0026\u0026 p.idle.back.t.Add(p.IdleTimeout).Before(nowFunc()); i++ { pc := p.idle.back p.idle.popBack() p.mu.Unlock() pc.c.Close() p.mu.Lock() p.active-- } } 因为，redigo 连接池的空闲连接链表 idle 是一个先进后出的链表，所以，back 所指向的空闲连接是相对较老的，所以，这里是从 idle.back 开始判断是否已经超时，如果超时，则将其从链表中取出，并且关闭该连接。 第三段代码是从 idle 中取出空闲的连接： for p.idle.front != nil { pc := p.idle.front p.idle.popFront() p.mu.Unlock() if (p.TestOnBorrow == nil || p.TestOnBorrow(pc.c, pc.t) == nil) \u0026\u0026 (p.MaxConnLifetime == 0 || nowFunc().Sub(pc.created) \u003c p.MaxConnLifetime) { return pc, nil } pc.c.Close() p.mu.Lock() p.active-- } 如上所说，它是从 idle 的 front 取出一个空闲的连接。如果有设置测试函数，会先执行测试；如果有设置MaxConnLifetime，那么将会判断连接的生命周期是否已经超过该值。如果能在 idle 中取得符合条件的连接，那么会将其返回，否则会将不符合的连接关闭直到取出符合条件的连接，或者直到 idle 中没有空闲的连接，那么将会继续往下执行。 再往下是两个判断： if p.closed { p.mu.Unlock() return nil, errors.New(\"redigo: get on closed pool\") } if !p.Wait \u0026\u0026 p.MaxActive \u003e 0 \u0026\u0026 p.active \u003e= p.MaxActive { p.mu.Unlock() return nil, ErrPoolExhausted } 第一个是判断连接池是否已经关闭。第二个是判断未设置 Wait 但是设置了 MaxActive 的情况下，已创建的连接数是否已经达到了 MaxActive，如果条件满足，则返回错误。 最后一段代码是创建并返回连接： p.active++ p.mu.Unlock() c, err := p.Dial() if err != nil { c = nil p.mu.Lock() p.active-- if p.ch != nil \u0026\u0026 !p.closed { p.ch \u003c- struct{}{} } p.mu.Unlock() } return \u0026poolConn{c: c, created: nowFunc()}, err 如果创建失败，那么，它会重新往 p.ch 中写入值。因为在第一段代码中从 p.ch 取出了一个值，如果创建失败了不重新写入，那么等于 p.ch 中能缓冲的数据就减少了，从而导致能从连接池中获取的连接就减少了，最后可能就在第一段代码那里死锁了。 ","date":"2019-06-23","objectID":"/redigo_pool.html:2:0","tags":["golang"],"title":"读源码:redigo 的连接池","uri":"/redigo_pool.html"},{"categories":null,"content":"连接的回收 在一个不使用连接池的场景中，当调用了 conn.Close() 之后，当前的连接将会被关闭，而在使用连接池的情况下，调用 conn.Close() 会发生什么呢？我们来看一下连接池返回的连接，也就是 activeConn 的 Close 方法的实现： func (ac *activeConn) Close() error { pc := ac.pc if pc == nil { return nil } ac.pc = nil if ac.state\u0026internal.MultiState != 0 { pc.c.Send(\"DISCARD\") ac.state \u0026^= (internal.MultiState | internal.WatchState) } else if ac.state\u0026internal.WatchState != 0 { pc.c.Send(\"UNWATCH\") ac.state \u0026^= internal.WatchState } if ac.state\u0026internal.SubscribeState != 0 { pc.c.Send(\"UNSUBSCRIBE\") pc.c.Send(\"PUNSUBSCRIBE\") // To detect the end of the message stream, ask the server to echo // a sentinel value and read until we see that value. sentinelOnce.Do(initSentinel) pc.c.Send(\"ECHO\", sentinel) pc.c.Flush() for { p, err := pc.c.Receive() if err != nil { break } if p, ok := p.([]byte); ok \u0026\u0026 bytes.Equal(p, sentinel) { ac.state \u0026^= internal.SubscribeState break } } } pc.c.Do(\"\") ac.p.put(pc, ac.state != 0 || pc.c.Err() != nil) return nil } 前面一部分代码都是和 redis 服务器进行通信，释放相关的资源，这里主要看 put 方法的实现： func (p *Pool) put(pc *poolConn, forceClose bool) error { p.mu.Lock() if !p.closed \u0026\u0026 !forceClose { pc.t = nowFunc() p.idle.pushFront(pc) if p.idle.count \u003e p.MaxIdle { pc = p.idle.back p.idle.popBack() } else { pc = nil } } if pc != nil { p.mu.Unlock() pc.c.Close() p.mu.Lock() p.active-- } if p.ch != nil \u0026\u0026 !p.closed { p.ch \u003c- struct{}{} } p.mu.Unlock() return nil } 首先，它将需要释放的连接插入了 idle 的头部。如果此时空闲的连接数超过了 MaxIdle，那么，将会把尾部的空闲连接即最老的空闲连接取出，并将其关闭。最后，往 p.ch 中写入值，保证在设置了 Wait 的情况下正常运行。 ","date":"2019-06-23","objectID":"/redigo_pool.html:3:0","tags":["golang"],"title":"读源码:redigo 的连接池","uri":"/redigo_pool.html"},{"categories":null,"content":"结语 redigo 的连接池就大概分享完了，时间比较匆忙，感觉总结的不是特别好，希望大家能多多斧正。 ","date":"2019-06-23","objectID":"/redigo_pool.html:4:0","tags":["golang"],"title":"读源码:redigo 的连接池","uri":"/redigo_pool.html"},{"categories":null,"content":"redigo是golang的一个操作redis的第三方库，之所以选择这个库，是因为它的文档十分丰富，操作起来也比较简单。一个典型的redigo的使用如下所示： package main import ( \"github.com/gomodule/redigo/redis\" \"log\" ) func main() { conn, err := redis.Dial(\"tcp\", \"192.168.1.2:6379\") if err != nil { log.Fatalf(\"dial redis failed :%v\\n\", err) } result, err := redis.String(conn.Do(\"SET\", \"hello\", \"world\")) if err != nil { log.Fatalln(err) } log.Println(result) } 这里需要注意的一点是，redis 默认是只能本机访问的，可以通过修改 /etc/redis/redis.conf 中的 bind 来实现远程访问，这里我将 bind 改为了服务所在机器的 IP 。 虽然，redigo 的使用十分简单，但是，在它的文档中也指出了一点需要我们特别注意，我们可以在 godoc 中看到原文： Connections support one concurrent caller to the Receive method and one concurrent caller to the Send and Flush methods. No other concurrency is supported including concurrent calls to the Do and Close methods. 翻译过来就是： 连接支持同时运行单个执行体调用 Receive 和 单个执行体调用 Send 和 Flush 方法。不支持并发调用 Do 和 Close 方法。 本着程序员追根究底的好奇心，我看了一下 redigo 实现 Do 方法的源码，大致弄清楚了为什么 Do 函数是并发不安全的了。它的部分源码如下所示： func (c *conn) Do(cmd string, args ...interface{}) (interface{}, error) { return c.DoWithTimeout(c.readTimeout, cmd, args...) } func (c *conn) DoWithTimeout(readTimeout time.Duration, cmd string, args ...interface{}) (interface{}, error) { c.mu.Lock() pending := c.pending c.pending = 0 c.mu.Unlock() if cmd == \"\" \u0026\u0026 pending == 0 { return nil, nil } if c.writeTimeout != 0 { c.conn.SetWriteDeadline(time.Now().Add(c.writeTimeout)) } if cmd != \"\" { if err := c.writeCommand(cmd, args); err != nil { return nil, c.fatal(err) } } if err := c.bw.Flush(); err != nil { return nil, c.fatal(err) } var deadline time.Time if readTimeout != 0 { deadline = time.Now().Add(readTimeout) } c.conn.SetReadDeadline(deadline) if cmd == \"\" { reply := make([]interface{}, pending) for i := range reply { r, e := c.readReply() if e != nil { return nil, c.fatal(e) } reply[i] = r } return reply, nil } var err error var reply interface{} for i := 0; i \u003c= pending; i++ { var e error if reply, e = c.readReply(); e != nil { return nil, c.fatal(e) } if e, ok := reply.(Error); ok \u0026\u0026 err == nil { err = e } } return reply, err } func (c *conn) writeCommand(cmd string, args []interface{}) error { c.writeLen('*', 1+len(args)) if err := c.writeString(cmd); err != nil { return err } for _, arg := range args { if err := c.writeArg(arg, true); err != nil { return err } } return nil } 上面三个函数实现在 redigo 的 redis 包的 conn.go 文件中，在 DoWithTimeout 方法中，我们可以看到它是顺序执行数据的发送和相应的接收的，而且，函数中还是没有加锁的。虽然，golang 的 TCP 发送底层实现是有加锁的，可以保证一次写操作的数据中，不会有另一次写操作的数据插入，但是，在这个 DoWithTimeout 的实现中，我们还是能隐约闻到一种不安全的味道。 我们把焦点锁定在 writeCommand 这个方法上。从它的实现，我们可以了解到，它的作用主要是在 for … range 中将 redis 的命令发送到 redis-server 执行。这时，我们可能会注意到，这个函数是没有加锁的，如果 for … range 是往一个全局的缓冲去中写数据，那么，并发时很有可能会导致数据的交叉。为了证实这个假设，我们继续看 writeArg 的实现: func (c *conn) writeArg(arg interface{}, argumentTypeOK bool) (err error) { switch arg := arg.(type) { case string: return c.writeString(arg) case []byte: return c.writeBytes(arg) case int: return c.writeInt64(int64(arg)) case int64: return c.writeInt64(arg) case float64: return c.writeFloat64(arg) case bool: if arg { return c.writeString(\"1\") } else { return c.writeString(\"0\") } case nil: return c.writeString(\"\") case Argument: if argumentTypeOK { return c.writeArg(arg.RedisArg(), false) } // See comment in default clause below. var buf bytes.Buffer fmt.Fprint(\u0026buf, arg) return c.writeBytes(buf.Bytes()) default: // This default clause is intended to handle builtin numeric types. // The function should return an error for other types, but this is not // done for compatibility with previous versions of the package. var buf bytes.Buffer fmt.Fprint(\u0026buf, arg) return c.writeBytes(buf.Bytes()) } } func (c *conn) writeString(s string) error { c.writeLen('$', len(s)) c.bw.WriteString(s) _, err := c.bw.WriteString(\"\\r\\n\") return err } writeArg 方法是通过判断传入参数的不同来调用不同的方法来写数据的，不过这几个方法的底层其实都是调用了 writeString 这个方法。在 writeString 这个方法的实现中，我们看到 redigo 是把数据都写到 bw 的。bw 是 conn 一个 net.Conn 的 writter，也就是说，如果并发执行 Do 方法的话，这几个并发的执行体都是往同一个 net.Conn的 writter 中写数据的，这基本证实了我上面的假设。 我们回过来看 DoWithTimeout 函数执行了 writeCommand 之后，调用的 bw 的 Flush ","date":"2019-06-16","objectID":"/why_redis_unsafe_in_concurrence.html:0:0","tags":["golang"],"title":"读源码:redigo为什么多线程不安全","uri":"/why_redis_unsafe_in_concurrence.html"},{"categories":["golang"],"content":"最近项目中有一个功能，是定时从远端服务器同步数据到本地，数据中有一个字段是时间格式的。每次我的同步程序从服务器上获取到数据和本地数据库保存的数据进行比较的时候，总是会提示数据发生了变化，但是事实上我并没有修改服务器上的数据。 为了查出问题出现的原因，我在同步的时候将从服务器获取的数据和本地数据库读取的数据都打印了出来进行对比，发现两个数据唯一的区别在于时间的时区不同：服务器获取的数据是 UTC，而本地数据库的数据用的是 CST。而服务器上获取的时间本是字符串的，是我使用 time.Parse 函数将其转换成时间的，那么问题可能就出在这个函数身上了。 找到方向后，我就看了下 time.Parse 的源码，发现它实际是调用了 time.parse 函数，而 time.parse 函数的第三个参数就是需要转换的时间默认的时区，而 time.Parse 将该参数赋为 UTC，这可能就是产生错误的原因了。我尝试将 time.Parse 替换成了 time.ParseInLocation，问题解决。 示例： package main import ( \"fmt\" \"time\" ) func main() { t := \"2019-10-10 10:10:10\" t1, _ := time.Parse(\"2006-01-02 15:04:05\", t) t2, _ := time.ParseInLocation(\"2006-01-02 15:04:05\", t, time.Local) fmt.Println(t1) fmt.Println(t2) fmt.Println(t1.Equal(t2)) } 输出结果： ","date":"2019-05-24","objectID":"/time_package_in_golang.html:0:0","tags":["golang"],"title":"golang 中 time 包的时区问题","uri":"/time_package_in_golang.html"},{"categories":["golang","编程杂记"],"content":"最近项目在使用 golang 开发，对于一直使用 c 开发的我来说 golang 有着十分强大的内置库，生态也还可以，使用起来确实十分的舒适，最重要的是不需要手动管理内存，让人感觉快乐了很多，哈哈。不过，在写项目的时候也遇到了些因为不熟悉语言特性而出现的问题，比如 for range。代码逻辑大致如下: type foo struct { a string b string } func main() { var F = []foo{ foo{\"1\", \"2\"}, foo{\"3\", \"4\"}, } M := make(map[string]*foo) for _, v := range F { M[v.a] = \u0026v } log.Printf(\"%v\\n\", M) } 这段代码的本意 是想遍历切片 F 并把其元素的地址赋给 map M。但是运行的时候发现最后 M 中所有的 value 都是相同的值。重新 review 一边之后，发现在 13~15 行，我一直都在把临时变量 v 的地址赋给 M，而 for 循环都结束之后，v 的值是 F 中最后一个元素，所以 M 中所有的 value 就都一样了，且都是 F 中的最后一个元素。 为了避免这个问题，可以在 for 循环中在创建一个临时变量来保存 F 中的元素，并将其赋值给 M： for _, v := range F { tmp = v M[v.a] = \u0026tmp } 或者不要使用 for range 的形式： for i := 0; i \u003c len(F); i++ { M[F[i].a] = \u0026F[i] } ","date":"2019-04-17","objectID":"/for_range_in_golang.html:0:0","tags":["golang"],"title":"golang 中 for range 的问题","uri":"/for_range_in_golang.html"},{"categories":["network"],"content":"最近发现在 aws 上有 12 个月免费使用的 ec2 主机，于是就注册了一个，顺便搭建了这个博客。但是，一个好的博客不能总是用 IP 来访问的嘛。于是就在 godaddy 上买了一个域名，并且在阿里云的云解析上注册了 dns 解析，之后就可以用域名来访问本博客了。这时候有个疑问就出来了，为什么购买了域名，并且注册了 dns 解析后就可以在本地浏览器访问博客了呢？ ","date":"2019-03-31","objectID":"/63.html:0:0","tags":["DNS"],"title":"初探域名解析","uri":"/63.html"},{"categories":["network"],"content":"浏览器如何找到域名对应的 IP 当我们在浏览器的地址栏输入某个域名发起请求时，操作系统会通过最近的 dns 服务器查找域名对应的 IP 地址，比如我现在是用的电脑配置的 dns 服务器为 8.8.8.8。如果电脑没有设置 dns 服务器，那么会默认向路由器发起查询请求。 最近的 dns 服务器收到查询请求后，会先在本地缓存查找是否有域名对应的信息，如果有的话，将会直接返回对应的域名信息，否则，dns 服务器会向根域名服务器发起请求。根域名服务器会从上到下查找域名信息，直到找到对应的域名信息或者找不到对应的域名信息返回错误。 操作系统收到 dns 服务器的应答后，会做出对应的行为，比如访问对应的服务器或者返回给浏览器对应的错误。 ","date":"2019-03-31","objectID":"/63.html:1:0","tags":["DNS"],"title":"初探域名解析","uri":"/63.html"},{"categories":["network"],"content":"域名服务器 上面提到一个概念叫做**根域名服务器。**那么，什么是域名服务器，什么是根域名服务器呢？ 为什么需要域名？ ","date":"2019-03-31","objectID":"/63.html:2:0","tags":["DNS"],"title":"初探域名解析","uri":"/63.html"},{"categories":["network"],"content":"域名 大家都知道，点分式的 IP 地址由 4 段数字和 3 的英文句点组成，比如 192.168.125.47，这样的地址有点不好记忆，如果换成 foobar.com 这样的域名就会比较好记忆了。而映射域名和 IP 地址之间关系的就是域名服务器了。 ","date":"2019-03-31","objectID":"/63.html:2:1","tags":["DNS"],"title":"初探域名解析","uri":"/63.html"},{"categories":["network"],"content":"域名服务器 域名服务器是通过一个表来记录域名和 IP 的映射关系的，表中大概有四个字段：域名，class，记录类型以及响应数据 域名：显而易见，这个字段是记录需要查找的域名的名字的。 class：用来识别网络的信息，比如互联网是用 IN 来表示的（现在似乎也只有互联网了）。 记录类型：用来表示域名对用何种类型的记录。比如，如果域名对应的是某个 IP，那么，它的记录类型就是 A (address)。除了 A 记录之外，还有很多其他的记录类型，大家可以通过该 WIKI 了解。 响应数据：该字段则是表示返回给客户端的对应信息。比如，通过域名访问本博客的话，返回的就是本博客的地址了。 ","date":"2019-03-31","objectID":"/63.html:2:2","tags":["DNS"],"title":"初探域名解析","uri":"/63.html"},{"categories":["network"],"content":"域名的层级结构 那么，根域名服务器又是什么呢？大家可能会想到，如果我把所有的域名映射信息都放在本地，那么不就不需要复杂的域名查询流程了吗？但是，互联网上的域名实在是太多了，不可能放在一台服务器上的，于是就有了分层的域名结构，而不同分层的域名信息就可以放在不同的服务器上了（虽然也可以放在同一台服务器上）。比如，本博客的 www.tech-seeker.cn 就分为四个层级，分别是 www 域、tech-seeker 域、cn 域以及根域。大家可能会疑惑：前面三个域好理解，根域怎么又是怎么回事？其实，一个完整的域名是以英文句点结尾的，如：www.tech-seeker.cn. 。这个英文句点就是代表根域，但是通常都会将其省略。 ","date":"2019-03-31","objectID":"/63.html:2:3","tags":["DNS"],"title":"初探域名解析","uri":"/63.html"},{"categories":["network"],"content":"小结 当设备访问某个域名时，它会向最近的 dns 服务器发起查询。最近的 dns 服务器找不到对用的域名信息的时候，它就会向根域名服务器发起查询，然后，根域名服务器会根据域名的分层来查找对应的域名信息。然后返回给最近的 dns 服务器，dns服务器会将域名信息缓存在本地并且返回给发起访问的设备。 这是我第一次发表技术博客，可能还是有点杂乱，后面会保持至少一周一篇的节奏分享工作或者学习上的所得和疑问。希望能在分享的同时，更加深刻地巩固自己的所得。 ","date":"2019-03-31","objectID":"/63.html:3:0","tags":["DNS"],"title":"初探域名解析","uri":"/63.html"},{"categories":null,"content":" 本文是对 redis 官方文档 Redis latency problems troubleshooting 的翻译，诸位看官可结合原文食用。 这个文档将会帮助你理解在使用 Redis 遇到延迟问题时发生了什么。 在这个上下文中，延迟指的是从客户端发送命令和接收到应答之间所消耗的时间。通常情况下，Redis 处理命令的时间十分短，在亚微妙之间，但是某些情况会导致高延迟。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:0:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"我很忙，给我清单 接下来的内容对于以低延迟方式运行 Redis 是非常重要的。然而，我理解我们都是很忙的，所以就以快速清单开始吧。如果你在尝试了下面这些步骤失败了，请回来阅读完整的文档。 确认你运行慢查询命令阻塞了服务。使用 Redis 慢日志功能确认这一点。 对于 EC2 用户，确认你在基于现代的 EC2 实例上使用 HVM，比如 m3.medium。否则 fork() 会非常慢。 内核的 Transparent huge pages 必须请用。使用 echo never \u003e /sys/kernel/mm/transparent_hugepage/enabled 来禁用他们，然后重启 Redis 进程。 如果你使用虚拟机，那么即使你没有对 Redis 做任何操作也可能会有内部延迟。使用 ./redis-cli –intrinsic-latency 100 来在你的运行环境中检查最小延迟。注意：你需要在服务端而不是客户端运行这个命令。 启用 Redis 的 Latency monitor 功能，这是为了得到你的 Redis 实例中的延迟事件和原因的可读数据。 通常，使用下表来进行持久化和延迟/性能之间的权衡，从更好的安全性到更好的延迟排序。 AOF + fsync always：这会非常慢，你应该只在你知道自己在做什么的情况下使用。 AOF + fsync every second：这会是一个很好的折衷方案。 AOF + fsync every second + no-appendfsync-on-rewrite 选项设置为 yes：这也是一个折衷方案，但它避免了在重写时执行 fsync ，降低了磁盘压力。 AOF + fsync never：这个方案将 fsync 交给了内核，得到了更小的磁盘压力和延迟峰值风险。 RDB：这个方案你将会有更广泛的权衡空间，它取决于你如何触发 RDB 持久化。 现在，我们花 15 分钟来了解下细节… ","date":"0001-01-01","objectID":"/translation_redis_latency.html:1:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"测量延迟 如果你遇到了延迟问题，你可能知道如何在你的程序中测量延迟，或者延迟现象十分明显。其实，redis-cli 可以在毫秒级内测量 Redis 服务的延迟，只要使用下面的命令： redis-cli --latency -h `host` -p `port` ","date":"0001-01-01","objectID":"/translation_redis_latency.html:2:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"使用 Redis 内置的延迟监控子系统 从 Redis 2.8.13 开始，Redis 提供了延迟监控功能，它通过对不同执行路径（译者注：这个概念可以在延迟监控文档查看）采样来检测服务阻塞的位置。这将会使调试本文档所说明的问题更加方便，所以我们建议尽快启用延迟监控。请查看延迟监控文档。 延迟监控采样和报告功能会使你更方便的找出 Redis 系统延迟的原因，不过我们还是建议你详细阅读本文以更好的理解 Redis 和延迟尖峰。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:3:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"延迟基准线 有一种延迟本身就是你运行 Redis 的环境的一部分，即操作系统内核以及–如果你使用了虚拟化–管理程序造成的延迟。 这种延迟无法消除，学习它是十分重要的，因为它是基准线，换句话说，因为内核和管理程序的存在，你不可能使 Redis 的延迟比在你的环境中运行的程序的延迟更低。 我们将这种延迟称为内部延迟，redis-cli 从 Redis 2.8.7 开始就可以测量它了。这是一个在基于 Linux 3.11.0 的入门级服务器上运行的例子。 注意：参数 100 是测试执行的秒数。执行测试的时间越长，我们越有可能发现延迟峰值。100 秒通常足够了，但是你可能想要运行多次不同时长的测试。请注意这个测试是 CPU 密集型的，将可能会使你的系统的一个核满载。 注意： redis-cli 在这个例子中需要运行在你运行 Redis 或者计划运行 Redis 的服务器上，而不是在客户端。在这个特殊模式下 redis-cli 根本不会连接 Redis 服务：它只是尝试测试内核不提供 CPU 时间去运行 redis-cli 进程自己的最大时长。 在上面的例子中，系统的内部延迟只是 0.115毫秒（或 115 微妙），这是一个好的消息，但是请记住，内部延迟可能会随运行时间变化和变化，这取决与系统的负载。 虚拟化环境表现将会差点，特别是当高负载或者受其他虚拟化环境影响。下面是在 Linode 4096 实例上运行 Redis 和 Apache 的结果： 这里我们有 9.7 毫秒的内部延迟：这意味着我们不能要求 Redis 做的比这个更好了。然而，在负载更高或有其它邻居的不同虚拟化环境中运行时，可以更容易得到更差的结果。我们可能在正常运行的系统中得到 40 毫秒的测试结果。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:4:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"受网络和通讯影响的延迟 客户端通过 TCP/IP 或者 Unix 域来连接到 Redis。1 Gbit/s 的网络的典型延迟是 200 微妙，使用 Unix 域 socket 的延迟可能低至 30 微妙。它实际上取决于你的网络和系统硬件。在通讯之上，系统添加了更多的延迟（由于线程调度，CPU 缓存，NUMA 配置，等等）。系统在虚拟环境中造成的延迟明显高于物理机。 结论是，即使 Redis 处理大部分命令只花费亚微秒级的时间，客户端和服务端之间大量往返的命令将必须为网络和系统相关延迟付出代价。 因此，高效的客户端会将多个命令组成流水线来减少往返的次数。这被服务器和大多数客户端支持。批量操作比如 MSET/MGET 也是为了这个目的。从 Redis 2.4 开始，一些命令还支持所有数据类型的可变参数。 这里是一些准则： 如果负担得起，使用物理机而不是 VM 来托管服务。 不要总是连接和断开连接（特不是基于 web 的应用）。尽可能地保持连接。 如果你的客户端和服务端在同一台机器上，使用 Unix 域套接字。 使用批量命令（MSET/MGET），或使用带可变参数的命令，而不是使用流水线操作。 比起一系列的单独命令，更应该选择流水线操作（如果可能）。 Redis 支持 Lua服务器端脚本，以涵盖不适合原始流水线操作的情况（例如，当命令的结果是下一个命令的输入时）。 在 Linux，用户可以通过进程设置（taskset），cgroups，实时优先级（chrt），NUMA 设置，或者使用低延迟内核等来实现更好的延迟。请注意 Redis 并不适合绑定在单个 CPU 核心上。Redis 会 fork 后台任务像 bgsave 或 AOF，这操作会十分消耗 CPU。这些任务必须永远不和主进程在同一个核心上运行。 在大多环境中，这些系统级的优化不是必须的。只有当你需要它们或者熟悉它们的时候再去做这些操作。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:5:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"Redis 的单线程特性 Redis 被设计为大部分情况下使用单线程。这意味着一个线程处理所有客户端的请求，它使用了多路复用的技术。这意味着 Redis 在每个是简单都只处理一个请求，所以所有的请求都是按顺序处理的。这很像 Node.js 的工作方式。然而，这两个产品通常都不会被认为很慢。这是因为他们处理任务的时间很短，不过主要是因为它们设计为不在系统调用阻塞，特别是从套接字读取数据或者往套接字写数据的时候。 我说 Redis 大多只用单线程，是因为从 Redis 2.4 开始，我们使用多线程去在后台执行一些慢 I/O 操作，主要是和磁盘 I/O 相关，但是这不改变 Redis 使用单线程处理所有请求的事实。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:6:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"慢查询导致的延迟 一个单线程的后果是当有一个慢请求时，所有其它的客户端将会等待它完成。当执行像 GET 或 SET 或 LPUSH 这样的通常命令时是没有问题的，这些命令的执行时间都是常数的（非常短）。然而，有几个命令操作了大量的元素，像 SORT，LREM，SUNION 和其它的命令。例如，去两个大集合的交集会花费非常多的时间。 所有命令的算法复杂度都有文档记录。一个好的实践是，当你使用不熟悉的命令时先系统地测试一下它。 如果你有延迟的顾虑，那么你不应该用慢查询处理有大量元素的值，或者你应该运行 Redis 的副本去运行慢查询。 可以使用 Redis 的慢日志功能来监控慢查询。 另外，你可以使用你喜欢的进程监控程序（top，htop，prstat，等等）去快速的检查主 Redis 进程 所消耗的 CPU。如果它很高，但流量却不高，那么它通常表示正在执行慢查询。 重点：一个非常常见的由执行慢查询导致延迟的原因是在生产环境执行 KEYS 命令。KEYS 命令在文档中指出只能用于调试目的。从 Redis 2.8 开始，引入了一些新的命令来迭代键空间和其它大集合，请查看 SCAN，SSCAN，HSCAN 和 ZSCAN 命令来获取更多的信息。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:7:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"由 fork 引发的延迟 为了在后台生成 RDB 文件或者在启用 AOF 持久化时重写 AOF 文件，Redis 必须 fork 一个进程。fork 操作（在主线程运行）会引起延迟。fork 是在类 Unix 系统中一个开销很高的操作，因为它涉及到复制大量与进程关联的对象。对于和虚拟内存相关联的页表尤其如此。 例如，在 Linux/AMD64 系统，内存被分为每页 4 kB。为了将虚拟地址转换为物理地址，每个进程保存了一个页表（实际是一棵树），它至少包含一个指向进程的每页地址空间的指针。所以，一个拥有 24 GB 的 Redis 实例需要 24 GB/4 KB*8 = 48 MB 的页表。 当 bgsave 执行的时候，实例将会被 fork，这将创建和拷贝 48 MB 的内存。它会花费时间和 CPU，特别是在虚拟机上创建和初始化大页面将会是非常大的开销。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:8:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"不同系统上 fork 的时间 现代硬件拷贝页表非常快，除了 Xen。这个问题不是出在 Xen 虚拟化，而是 Xen 本身。例如，使用 VMware 或 Virtual Box 不会导致缓慢的 fork 时间。下表是比较不同的 Redis 实例 fork 所消耗的时间。数据来自于执行BGSAVE，并观察INFO命令输出的latest_fork_usec信息。 然而，好消息是基于 EC2 HVM 的实例执行 fork 操作的表现很好，几乎和在物理机上执行差不多，所以使用 m3.medium （或高性能）的实例将会得到更好的结果。 运行在 VMware 上的虚拟 Linux 系统 fork 6.0 GB 的 RSS 花费 77毫秒（每 GB 12.8 毫秒）。 Linux 运行在物理机上（未知硬件）fork 6.1 GB 的 RSS 花费了 80 毫秒（每 GB 13.1 毫秒）。 Linux 运行在物理机上（Xeon @ 2.27 Ghz）fork 6.9 GB 的 RSS 花费 62 毫秒（每 GB 9 毫秒）。 运行在 6sync 上的 Linux 虚拟机（KVM）fork 360 MB 的 RSS 花费 8.2 毫秒（每 GB 23.3 毫秒）。 运行在老版本 EC2 上的 Linux 虚拟机（Xen）fork 6.1 GB 的 RSS 花费 1460 毫秒（每 GB 10 毫秒）。 运行在新版本 EC2 上的 Linux 虚拟机（Xen）fork 1 GB 的 RSS 花费 10 毫秒（每 GB 10 毫秒）。 运行在 Linode 上的 Linux 虚拟机（Xen）fork 0.9 GB 的 RSS 花费 382 毫秒（每 GB 424 毫秒）。 正如您所看到的，在 Xen 上运行的某些虚拟机的性能损失介于一个数量级到两个数量级之间。对于 EC2 用户，建议很简单：使用基于 HVM 的现代实例。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:9:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"transparent huge pages引起的响应延迟 很不幸，如果 Linux 内核开启了 transparent huge pages 功能，Redis 将会在调用 fork 来持久化到磁盘时造成非常大的延迟。大内存页导致了下面这些问题： 当调用fork时，共享大内存页的两个进程将被创建。 在一个忙碌的实例上，一些事件循环就将导致访问上千个内存页，导致几乎整个进程执行写时复制。 这将导致高响应延迟和大内存的使用。 请确认使用以下命令禁用 transparent huge pages： echo never \u003e /sys/kernel/mm/transparent_hugepage/enabled ","date":"0001-01-01","objectID":"/translation_redis_latency.html:10:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"页面交换（操作系统分页）引起的延迟 Linux（还有很多其它现代操作系统）可以将内存页从内存缓存到磁盘，或从磁盘读入内存，这是为了更有效的使用系统内存。 如果 Redis 页被内核从内存保存到了交换文件，当保存在这个内存页中的数据被 Redis 使用到的时候（比如访问保存在这个页中的键）内核为了将这页移动到主存，会停止 Redis 进程。访问随机 I/O 是一个缓慢的操作（和访问在内存中的页相比）而且 Redis 客户端将会经历异常的延迟。 内核将 Redis 内存页保存到磁盘主要有三个原因： 系统正在内存压力之下，因为运行的进程要求了比可用内存更多的物理内存。最简单的例子是 Redis 使用了比可用内存更多的内存。 Redis 实例的数据集合或一部分数据集合已经基本完全空闲（从未被客户端访问），所以内核可能会将空闲的内存页保存到磁盘。这个问题十分罕见，因为即使是一个不是特别慢的实例通常也会访问所有的内存页，强迫内核将所有的页保存在内存中。 系统中的一些进程引发了大量读写 I/O 的操作。因为文件会产生缓存，它会给内核增加文件系统缓存的压力，因此就会产生交换活动。请注意，这包括 Redis RDB 和/或 AOF 线程，它们会产生大文件。 幸运的是 Linux 提供了好的工具去验证这个问题，所以，最简单的事是当怀疑是交换内存引起延迟时，那就去检查它吧。 第一件事要做的是检查 Redis 内存交换到磁盘的数量。这之前，你需要获取 Redis 实例的 pid： $ redis-cli info | grep process_id process_id:5454 现在进入 /proc 目录下该进程的目录： $ cd /proc/5454 你将会在这里找到一个叫做 smaps 的文件，它描述了 Redis 进程的内存布局（假设你使用的是 Linux 2.6.16 以上的系统）。这个文件包含了和我们的进程内存映射相关的非常详细的信息，一个名为 Swap 的字段就是我们所需要的。然而，当 smaps 文件包含了不同的关于 Redis 进程的内存映射之后，它就不是单单一个 swap 字段了（进程的内存布局比一页简单的线性表要远远复杂）。 因为我们对进程相关的内存交换都感兴趣，所以第一件事要做的是用 grep 得到文件中所有的 Swap 字段： $ cat smaps | grep 'Swap:' Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 12 kB Swap: 156 kB Swap: 8 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 4 kB Swap: 0 kB Swap: 0 kB Swap: 4 kB Swap: 0 kB Swap: 0 kB Swap: 4 kB Swap: 4 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB Swap: 0 kB 如果所有都是 0 KB，或者零散一个 4k 的条目，那么一切都很完美。事实上，在我们的示例中（一个运行 Redis 的网站，每秒服务百余用户）有些条目会显示更多的交换页。为了研究这是否是一个严重的问题，我们更改命令以便打印内存映射的大小： $ cat smaps | egrep '^(Swap|Size)' Size: 316 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 8 kB Swap: 0 kB Size: 40 kB Swap: 0 kB Size: 132 kB Swap: 0 kB Size: 720896 kB Swap: 12 kB Size: 4096 kB Swap: 156 kB Size: 4096 kB Swap: 8 kB Size: 4096 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 1272 kB Swap: 0 kB Size: 8 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 16 kB Swap: 0 kB Size: 84 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 8 kB Swap: 4 kB Size: 8 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 4 kB Swap: 4 kB Size: 144 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 4 kB Swap: 4 kB Size: 12 kB Swap: 4 kB Size: 108 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 4 kB Swap: 0 kB Size: 272 kB Swap: 0 kB Size: 4 kB Swap: 0 kB 正如你从输出所见，这里有一个映射有 720896 kB（只有 12 KB 被交换），在另一个映射中有 156 KB 被交换：基本上占我们内存非常少的部分被交换，所以不会造成大的问题。 相反，如果有大量进程内存页被交换到磁盘，那么你的响应延迟问题可能和内存页交换有关。如果是这样的话，你可以使用vmstat命令来进一步检查你的 Redis 实例： 输出中我们需要的一部分是两列 si 和 so ，它们记录了内存从 swap 文件交换出和交换入的次数。如果你看到的这两列是非 0 的，那么你的系统上存在内存交换。 最后， iostat 命令可以用于检测系统的全局 I/O 活动。 如果延迟问题是由于Redis内存在磁盘上交换造成的，则需要降低系统内存压力，如果Redis使用的内存超过可用内存，则增加更多内存，或者避免在同一系统中运行其他内存耗尽的进程。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:11:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"由于AOF和磁盘I / O导致的延迟 另一个导致延迟的原因是 Redis 支持的 AOF 功能。AOF 基本上只使用两个系统调用来完成工作。一个是 write(2)，它用来追加数据到文件，另一个是 fdatasync(2) ，用来刷新内核文件缓存到磁盘，用来确保用户指定的持久化级别。 write(2)和 fdatasync(2)都有可能导致延迟。例如，当正在进行系统大范围同步，或者输出缓冲区已满，内核需要将数据刷新到磁盘来保证接受新的写操作时，都有可能阻塞 write(2)。 fdatasync(2)调用是一个导致延迟的更糟糕的原因，因为使用的内核和文件系统的许多组合可能需要几毫秒到几秒才能完成，特别是在某些其他进程执行 I/O 的情况下。出于这个原因，Redis 2.4 可能会在不同的线程中执行fdatasync(2)调用。 我们将会看到如何设置可以改善使用 AOF 文件引起的延迟问题。 可以使用appendfsync配置选项将 AOF配 置为以三种不同方式在磁盘上执行 fsync（可以使用CONFIG SET命令在运行时修改此设置）。 当 appendfsync 被设置为 no时，Redis 将不会执行 fsync。这种设置方式里，只有write(2)会导致延迟。这种情况下发生延迟通常没有解决方法，原因非常简单，因为磁盘拷贝的速度无法跟上 Redis 接收数据的速度，不过这种情况十分不常见，除非因为其他进程在操作 I/O 导致磁盘读写很慢。 当 appendfsync被设置为 everysec 时，Redis 每秒执行一次 fsync。它会使用一个不同的线程，并且当 fsync 在运行的时候，Redis 会使用 buffer 来延迟 write(2) 的调用 2 秒左右（因为在 Linux 中，当 write 和 fsync 在进程中竞争同一个文件时会导致阻塞）。然而，如果 fsync 占用了太长的时间，Redis 最终会执行 write 调用，这将会导致延迟。 当 appendfsync 被设置为 always 时，fsync 会在每次写操作时执行，这个操作发生在回复 OK 应答给客户端之前（事实上 Redis 会尝试将多个同时执行的命令集合到一个 fsync 中）。在这种模式下性能通常会非常慢，非常推荐使用快的磁盘和执行 fsync 很快的文件系统。 大多数 Redis 用户会使用 no 或者 everysec 来设置 appendfsync。获得最少的延迟的建议时避免其它进程在同一系统中操作 I/O。使用 SSD 硬盘可以提供很好的帮助，不过如果磁盘是空闲的，那么非 SSD 磁盘也能有很好的表现，因为 Redis 写 AOF 文件的时候不需要执行任何查找。 如果想要确认延迟是否和 AOF 文件相关，在 Linux 下你可以使用 strace 命令来查看： sudo strace -p $(pidof redis-server) -T -e trace=fdatasync 上面的命令将会显示 Redis 在主线程执行的所有 fdatasync(2) 命令。你不能通过上面的命令查看当 appendfsync 设置为 everysec时在后台线程执行的 fdatasync命令。可以添加 -f 参数来查看后台执行的 fdatasync命令。 如果你想要同时查看 fdatasync 和 write 两个系统调用，可以使用下面的命令: sudo strace -p $(pidof redis-server) -T -e trace=fdatasync,write 不过，因为 write命令还被用于写数据到客户端的套接字，所以可能会显示很多和磁盘 I/O 无关的数据。显然没有办法告诉 strace 只显示慢速系统调用，所以我使用以下命令： sudo strace -f -p $(pidof redis-server) -T -e trace=fdatasync,write 2\u003e\u00261 | grep -v '0.0' | grep -v unfinished ","date":"0001-01-01","objectID":"/translation_redis_latency.html:12:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"到期产生的延迟 Redis 使用下面两种方法来处理过期的键： 一种被动的方式是，在一个命令被请求时，如果发现它已经过期了，那就将它删除。 一种主动的方式是，每 100 毫秒删除一些过期的键。 主动过期的方式被设计为自适应的。每 100 毫秒一个周期（每秒 10 次），它将进行下面的操作： 采用ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 键，删除所有已经过期的键。 如果超过 25% 的键是已经过期的，重复这个过程。 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 的默认值是 20，这个过程每秒执行 10 次，通常没有将会有 200 个键因过期被主动删除。这已经能够快地清除 DB 了，即使有些键已经很久没有被访问了，所以被动算法是没什么帮助的。同时，每秒删除 200 个键并不会对 Redis 的延迟造成影响。 然而，这个算法是自适应的，如果在采样键集合中找到了超过 25% 的键是过期的，它会循环执行。但是运行这个算法的周期是每秒 10 次，这意味着可能发生在同一秒内被采样的键 25% 以上都过期的情况。 基本上，这意味着 如果数据库在同一秒内有非常多键过期了，而且它们构成了当前键集合 25% 的失效键集合，Redis 可能会阻塞着直到失效键的比例降到 25% 以下。 这种方式是为了避免因为失效的键使用过多的内存，而且通常都是无害的，因为在同一秒内出现大量失效的键是十分奇怪的，但是用户在同一 Unix 时间广泛使用 EXPIREAT 命令也不是不可能。 简而言之：注意大量键同时过期引起的响应延迟。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:13:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"},{"categories":null,"content":"Redis 软件看门狗 Redis 2.6 引入了 Redis 软件看门狗这个调试工具，它被设计用来跟踪使用其它通常工具无法分析的导致延迟的问题。 软件看门狗是一个试验性的特性。虽然它被设计为用于开发环境，不过在继续使用之前还是应该备份数据库，因为它可能会对 Redis 服务器的正常操作造成不可预知的影响。 它只有在通过其它方法没有办法找到原因的情况下才能使用。 下面是这个特性的工作方式： 用户使用 CONFIG SET 命令启用软件看门狗。 Redis 开始一直监控自己。 如果 Redis 检测到服务器阻塞在了某些操作中，没有即使返回，并且这可能是导致延迟的原因，一个低层的关于服务器阻塞位置的报告将会生成到日志文件中。 用户在 Redis 的 Google Group 中联系开发者，并展示监控报告的内容。 注意，这个特性不能在 redis.conf文件中启用，因为它被设计为只能在已经运行的实例中启用，并且只能用于测试用途。 使用下面的命令来启用这个特性： CONFIG SET watchdog-period 500 period 被指定为毫秒级。上面这个例子是指在检测到服务器有 500 毫秒或以上的延迟时则记录到日志文件中。最小可配置的延迟时间是 200 毫秒。 当你使用完了软件看门狗，你可以将watchdog-period参数设置为 0 来关闭这一功能。 重点：一定要记得关闭它，因为长时间开启看门狗不是一个好的注意。 下面的内容是当看门狗检测到延迟大于设置的值时会记录到日志文件中的内容： [8547 | signal handler] (1333114359) --- WATCHDOG TIMER EXPIRED --- /lib/libc.so.6(nanosleep+0x2d) [0x7f16b5c2d39d] /lib/libpthread.so.0(+0xf8f0) [0x7f16b5f158f0] /lib/libc.so.6(nanosleep+0x2d) [0x7f16b5c2d39d] /lib/libc.so.6(usleep+0x34) [0x7f16b5c62844] ./redis-server(debugCommand+0x3e1) [0x43ab41] ./redis-server(call+0x5d) [0x415a9d] ./redis-server(processCommand+0x375) [0x415fc5] ./redis-server(processInputBuffer+0x4f) [0x4203cf] ./redis-server(readQueryFromClient+0xa0) [0x4204e0] ./redis-server(aeProcessEvents+0x128) [0x411b48] ./redis-server(aeMain+0x2b) [0x411dbb] ./redis-server(main+0x2b6) [0x418556] /lib/libc.so.6(__libc_start_main+0xfd) [0x7f16b5ba1c4d] ./redis-server() [0x411099] ------ 注意：在例子中 DEBUG SLEEP 命令是用来组在服务器的。如果服务器阻塞在不同的位置，那么堆栈信息将会是不同的。 如果你碰巧收集了多份看门狗堆栈信息，我们鼓励你将他们都发送到 Redis 的 Google Group：我们收集到越多的堆栈信息，那么你的实例的问题将越容易被理解。 ","date":"0001-01-01","objectID":"/translation_redis_latency.html:14:0","tags":["redis","translate"],"title":"译-redis延迟问题排查","uri":"/translation_redis_latency.html"}]